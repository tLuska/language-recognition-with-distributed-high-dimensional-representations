{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import itertools\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_base(alphabet, n_order):\n",
    "    assert type(alphabet) == list, 'Alphabet is not list.'\n",
    "    assert len(set(alphabet)) == len(alphabet), 'Alphabet is not correct.'\n",
    "    alphabet.sort()\n",
    "    n_gram_base = set()\n",
    "    for combination in itertools.combinations_with_replacement(alphabet, n_order):\n",
    "        for permutation in itertools.permutations(combination, n_order):\n",
    "            n_gram = ''\n",
    "            for letter in permutation:\n",
    "                n_gram += letter\n",
    "            n_gram_base.add(n_gram)\n",
    "    assert len(n_gram_base) == len(alphabet)**n_order, 'Incorrect result'\n",
    "    print(\"Created\", str(n_order)+\"-gram base with\", len(n_gram_base), str(n_order)+\"-grams.\")\n",
    "    n_gram_base = list(n_gram_base)\n",
    "    n_gram_base.sort()\n",
    "    return n_gram_base\n",
    "\n",
    "def alphabet():\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    alphabet.append(' ')\n",
    "    return alphabet\n",
    "\n",
    "def process_sentenses(path_to_file):\n",
    "    alph = alphabet()\n",
    "    delimiters = \"\\t\", \"\\n\"\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    regexPattern\n",
    "\n",
    "    raw_lines = []\n",
    "    with open(path_to_file, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            raw_lines.append(line)\n",
    "        \n",
    "    lines_split = [re.split(regexPattern, line)[1].lower() for line in raw_lines]\n",
    "    \n",
    "    lines_final = []\n",
    "\n",
    "    for i in tqdm(range(len(lines_split))):\n",
    "        line = lines_split[i]\n",
    "        line_ = [unidecode.unidecode(x) for x in line if x.isalpha() or x.isspace()]\n",
    "        \n",
    "        line_ = [x for x in ''.join(line_).split(' ') if x]\n",
    "        new_line = []\n",
    "        for word in line_:\n",
    "            word = [x for x in word if x in alph]\n",
    "            new_line.append(''.join(word).lower())\n",
    "        lines_final.append(new_line)\n",
    "    del delimiters, regexPattern, raw_lines, lines_split, line, line_\n",
    "    return lines_final\n",
    "\n",
    "def create_blocks(lines):    \n",
    "    blocks = []\n",
    "    current_block = ''\n",
    "    \n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = ' '.join(lines[i])\n",
    "        if len(current_block) < 1000:\n",
    "            if len(current_block) > 0:\n",
    "                if current_block[-1] != ' ': current_block += ' '\n",
    "            current_block += line\n",
    "        elif len(current_block) >= 1000:\n",
    "            blocks.append(current_block[:1000])\n",
    "            current_block = line\n",
    "    del current_block, line\n",
    "    return blocks\n",
    "\n",
    "def base_26_latin_ngrams(n):\n",
    "    alb = alphabet()\n",
    "    base_ngrams = create_ngram_base(alb, n)\n",
    "    del alb\n",
    "    return base_ngrams\n",
    "\n",
    "def create_rand_atom_vect(order=1000):\n",
    "    atom = np.random.randint(-1, 1, order)\n",
    "    atom = np.where(atom==0, 1, atom)\n",
    "    atom.reshape(-1,1)\n",
    "    return atom\n",
    "\n",
    "def create_item_memory(order=1000, alphabet_size=27):\n",
    "    item_memory = []\n",
    "    for i in range(alphabet_size):\n",
    "        item_memory.append(create_rand_atom_vect(order))\n",
    "    item_memory = np.array(item_memory)\n",
    "    return item_memory    \n",
    "\n",
    "def create_fixed_permutations(order=1000, n=3):\n",
    "    index = [i for i in range(order)]\n",
    "    permutations = []\n",
    "    for i in range(n):\n",
    "        np.random.shuffle(index)\n",
    "        permutations.append(index.copy())\n",
    "    return permutations\n",
    "\n",
    "def bind(hdv1, hdv2):\n",
    "    assert type(hdv1) == np.ndarray and type(hdv2) == np.ndarray, 'Wrong HD vectors format'\n",
    "    binding = hdv1*hdv2\n",
    "    return binding\n",
    "    \n",
    "def permute(hdv, permutation, n_times_to_permute):\n",
    "    assert type(hdv) == np.ndarray, 'Wrong HD vector format'\n",
    "    assert type(permutation) == list, 'Wrong permutation format'\n",
    "    assert len(hdv) == len(permutation), 'Inconsistent vector/permutation shape'\n",
    "    for i in range(n_times_to_permute):\n",
    "        hdv = hdv[permutation]\n",
    "    return hdv\n",
    "    \n",
    "def bundle(hdv1, hdv2):\n",
    "    assert type(hdv1) == np.ndarray and type(hdv2) == np.ndarray, 'Wrong HD vectors format'\n",
    "    assert len(hdv1) == len(hdv2), 'Inconsistent vectors shape'\n",
    "    bundling = hdv1 + hdv2\n",
    "    return bundling\n",
    "\n",
    "def measure_cosine_similarity(hdv1, hdv2):\n",
    "    return(cosine_similarity([hdv1], [hdv2]))\n",
    "\n",
    "def jobsave(object_to_save, filename):\n",
    "    joblib.dump(object_to_save, filename)\n",
    "\n",
    "def jobload(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root = os.path.join('data', 'train')\n",
    "data_lang_paths = os.listdir(train_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bul_newscrawl_2017_1M',\n",
       " 'ces_newscrawl_2019_1M',\n",
       " 'dan_newscrawl_2019_1M',\n",
       " 'deu_newscrawl-public_2018_1M',\n",
       " 'ell_newscrawl_2017_1M',\n",
       " 'eng_newscrawl-public_2018_1M',\n",
       " 'est_newscrawl_2017_1M',\n",
       " 'fin_newscrawl_2017_1M',\n",
       " 'fra_newscrawl-public_2019_1M',\n",
       " 'hun_newscrawl_2017_1M',\n",
       " 'ita_newscrawl_2019_1M',\n",
       " 'lav_newscrawl_2016_1M',\n",
       " 'lit_newscrawl_2016_1M',\n",
       " 'nld_newscrawl_2019_1M',\n",
       " 'pol_newscrawl_2018_1M',\n",
       " 'por_newscrawl_2018_1M',\n",
       " 'ron_newscrawl_2015_1M',\n",
       " 'slk_newscrawl_2016_1M',\n",
       " 'slv_newscrawl_2016_1M',\n",
       " 'spa_newscrawl_2015_1M',\n",
       " 'swe_newscrawl_2018_1M']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths = defaultdict()\n",
    "for path in data_lang_paths:\n",
    "    lang_paths[path[:3]] = os.path.join(train_data_root, path, path+'-sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'bul': 'data\\\\train\\\\bul_newscrawl_2017_1M\\\\bul_newscrawl_2017_1M-sentences.txt',\n",
       "             'ces': 'data\\\\train\\\\ces_newscrawl_2019_1M\\\\ces_newscrawl_2019_1M-sentences.txt',\n",
       "             'dan': 'data\\\\train\\\\dan_newscrawl_2019_1M\\\\dan_newscrawl_2019_1M-sentences.txt',\n",
       "             'deu': 'data\\\\train\\\\deu_newscrawl-public_2018_1M\\\\deu_newscrawl-public_2018_1M-sentences.txt',\n",
       "             'ell': 'data\\\\train\\\\ell_newscrawl_2017_1M\\\\ell_newscrawl_2017_1M-sentences.txt',\n",
       "             'eng': 'data\\\\train\\\\eng_newscrawl-public_2018_1M\\\\eng_newscrawl-public_2018_1M-sentences.txt',\n",
       "             'est': 'data\\\\train\\\\est_newscrawl_2017_1M\\\\est_newscrawl_2017_1M-sentences.txt',\n",
       "             'fin': 'data\\\\train\\\\fin_newscrawl_2017_1M\\\\fin_newscrawl_2017_1M-sentences.txt',\n",
       "             'fra': 'data\\\\train\\\\fra_newscrawl-public_2019_1M\\\\fra_newscrawl-public_2019_1M-sentences.txt',\n",
       "             'hun': 'data\\\\train\\\\hun_newscrawl_2017_1M\\\\hun_newscrawl_2017_1M-sentences.txt',\n",
       "             'ita': 'data\\\\train\\\\ita_newscrawl_2019_1M\\\\ita_newscrawl_2019_1M-sentences.txt',\n",
       "             'lav': 'data\\\\train\\\\lav_newscrawl_2016_1M\\\\lav_newscrawl_2016_1M-sentences.txt',\n",
       "             'lit': 'data\\\\train\\\\lit_newscrawl_2016_1M\\\\lit_newscrawl_2016_1M-sentences.txt',\n",
       "             'nld': 'data\\\\train\\\\nld_newscrawl_2019_1M\\\\nld_newscrawl_2019_1M-sentences.txt',\n",
       "             'pol': 'data\\\\train\\\\pol_newscrawl_2018_1M\\\\pol_newscrawl_2018_1M-sentences.txt',\n",
       "             'por': 'data\\\\train\\\\por_newscrawl_2018_1M\\\\por_newscrawl_2018_1M-sentences.txt',\n",
       "             'ron': 'data\\\\train\\\\ron_newscrawl_2015_1M\\\\ron_newscrawl_2015_1M-sentences.txt',\n",
       "             'slk': 'data\\\\train\\\\slk_newscrawl_2016_1M\\\\slk_newscrawl_2016_1M-sentences.txt',\n",
       "             'slv': 'data\\\\train\\\\slv_newscrawl_2016_1M\\\\slv_newscrawl_2016_1M-sentences.txt',\n",
       "             'spa': 'data\\\\train\\\\spa_newscrawl_2015_1M\\\\spa_newscrawl_2015_1M-sentences.txt',\n",
       "             'swe': 'data\\\\train\\\\swe_newscrawl_2018_1M\\\\swe_newscrawl_2018_1M-sentences.txt'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [02:31<00:00, 6591.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:05<00:00, 15349.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:12<00:00, 13768.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:13<00:00, 13623.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [03:03<00:00, 5450.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:15<00:00, 13277.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:20<00:00, 12476.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:12<00:00, 13789.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:19<00:00, 12613.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:37<00:00, 10271.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:19<00:00, 12585.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:20<00:00, 12405.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:22<00:00, 12107.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:59<00:00, 16892.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:09<00:00, 14321.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:24<00:00, 11829.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:22<00:00, 12079.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:16<00:00, 13037.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [04:22<00:00, 3803.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:26<00:00, 11511.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [01:03<00:00, 15652.54it/s]\n"
     ]
    }
   ],
   "source": [
    "lines_raw = defaultdict()\n",
    "for lang in lang_paths:\n",
    "    lines_raw[lang] = process_sentenses(lang_paths[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 354724.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:01<00:00, 602374.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 405260.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 376549.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:03<00:00, 298094.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 388254.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:01<00:00, 570713.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 437235.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:04<00:00, 249512.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 396355.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:04<00:00, 226106.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 438562.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:03<00:00, 323047.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:04<00:00, 217942.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 435209.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:04<00:00, 211099.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 361248.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:01<00:00, 656065.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:01<00:00, 811972.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:01<00:00, 516823.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:02<00:00, 475389.87it/s]\n"
     ]
    }
   ],
   "source": [
    "lang_blocks = defaultdict()\n",
    "for lang in lines_raw:\n",
    "    lang_blocks[lang] = create_blocks(lines_raw[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "fname = 'lang_blocks_train.jbl'\n",
    "jobsave(lang_blocks, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fname = 'lang_blocks_train.jbl'\n",
    "lang_blocks_train = jobload(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Encode train data using HD vectors, 3-grams and 21 languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 create latin alphabet and item memory using 27 atom HD binary (-1, 1) vectors with size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 26 letter + 1 space symbol alphabet:\n",
    "latin_alphabet = alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27, 1000), -90)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create item memory\n",
    "item_memory = create_item_memory(order=1000, alphabet_size=27)\n",
    "\n",
    "# check the shape; sum of item memory can show if some bias exist in random destribution\n",
    "item_memory.shape, np.sum(item_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 create fixed permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_permutations = create_fixed_permutations(order=1000, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 encode hd vector representations for all possible 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3-gram base with 19683 3-grams.\n"
     ]
    }
   ],
   "source": [
    "base_ngrams = base_26_latin_ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_alphabet = alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aja\n",
      "0\n",
      "(1000,)\n",
      "9\n",
      "(1000,)\n",
      "0\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(base_ngrams[1000])\n",
    "for letter in base_ngrams[1000]:\n",
    "    ind = latin_alphabet.index(letter)\n",
    "    print(ind)\n",
    "    print(item_memory[ind].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ngram(ngram):\n",
    "    vectors = []\n",
    "    for order in range(len(ngram)):\n",
    "        letter = ngram[order]\n",
    "        item_id = latin_alphabet.index(letter)\n",
    "        \n",
    "        # this is atom vector for that simbol\n",
    "        item_itself = item_memory[item_id]\n",
    "        permutation_n = fixed_permutations[order]\n",
    "        permuted_hdv = permute(item_itself, permutation_n, order)\n",
    "        vectors.append(permuted_hdv)\n",
    "    output_vector = vectors[0]\n",
    "    for hdv in vectors[1:]:\n",
    "        output_vector = bind(output_vector, hdv)\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_trigrams = defaultdict()\n",
    "for trigram in base_ngrams:\n",
    "    encoded_trigrams[trigram] =encode_ngram(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "jobsave(item_memory, 'item_memory.jbl')\n",
    "jobsave(fixed_permutations, 'fixed_permutations.jbl')\n",
    "jobsave(base_ngrams, 'base_ngrams.jbl')\n",
    "jobsave(encoded_trigrams, 'encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "item_memory = jobload('item_memory.jbl')\n",
    "fixed_permutations = jobload('fixed_permutations.jbl')\n",
    "base_ngrams = jobload('base_ngrams.jbl')\n",
    "encoded_trigrams = jobload('encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Unsing encoded n-grams, encode the training data blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_encode_text_block(data_block, encoded_trigrams):\n",
    "\n",
    "    tri_grams = [data_block[i:i+3] for i in range(len(data_block)-2)]\n",
    "    encoded_block_trigrams = []\n",
    "    for tg in tri_grams:\n",
    "        encoded_block_trigrams.append(encoded_trigrams[tg.lower()])\n",
    "    \n",
    "    enc_tg_start = encoded_block_trigrams[0]\n",
    "    for enc_tg in encoded_block_trigrams:\n",
    "        enc_tg_start = bundle(enc_tg_start, enc_tg)\n",
    "    return enc_tg_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 59/99553 [00:00<02:51, 580.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 99553/99553 [03:24<00:00, 486.47it/s]\n",
      "  0%|                                                                             | 106/78026 [00:00<02:29, 520.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 78026/78026 [02:25<00:00, 538.00it/s]\n",
      "  0%|                                                                             | 111/96993 [00:00<02:56, 548.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 96993/96993 [02:57<00:00, 545.05it/s]\n",
      "  0%|                                                                             | 109/96488 [00:00<02:59, 538.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 96488/96488 [02:56<00:00, 548.02it/s]\n",
      "  0%|                                                                             | 57/116989 [00:00<03:28, 560.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 116989/116989 [03:30<00:00, 556.41it/s]\n",
      "  0%|                                                                             | 53/103681 [00:00<03:17, 523.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 103681/103681 [03:35<00:00, 482.22it/s]\n",
      "  0%|                                                                             | 48/101016 [00:00<03:33, 472.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 101016/101016 [03:31<00:00, 476.63it/s]\n",
      "  0%|                                                                              | 48/93543 [00:00<03:16, 476.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 93543/93543 [03:14<00:00, 480.46it/s]\n",
      "  0%|                                                                            | 110/105407 [00:00<03:11, 548.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 105407/105407 [03:36<00:00, 487.68it/s]\n",
      "  0%|                                                                             | 93/113367 [00:00<04:08, 455.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 113367/113367 [04:00<00:00, 470.46it/s]\n",
      "  0%|                                                                             | 50/112002 [00:00<03:48, 489.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ita\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 112002/112002 [03:57<00:00, 471.16it/s]\n",
      "  0%|                                                                             | 43/100529 [00:00<03:55, 426.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100529/100529 [03:35<00:00, 465.67it/s]\n",
      "  0%|                                                                             | 48/100053 [00:00<03:28, 479.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100053/100053 [03:20<00:00, 500.21it/s]\n",
      "  0%|                                                                             | 113/86687 [00:00<02:36, 552.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 86687/86687 [02:40<00:00, 540.83it/s]\n",
      "  0%|                                                                              | 53/93810 [00:00<02:58, 526.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 93810/93810 [02:55<00:00, 535.62it/s]\n",
      "  0%|                                                                            | 114/106316 [00:00<03:08, 562.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "por\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 106316/106316 [03:16<00:00, 541.68it/s]\n",
      "  0%|                                                                             | 54/111425 [00:00<03:26, 538.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 111425/111425 [03:28<00:00, 535.07it/s]\n",
      "  0%|                                                                             | 105/97539 [00:00<03:08, 516.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 97539/97539 [03:08<00:00, 516.15it/s]\n",
      "  0%|                                                                            | 108/101224 [00:00<03:07, 538.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 101224/101224 [03:32<00:00, 476.60it/s]\n",
      "  0%|                                                                             | 46/122227 [00:00<04:28, 454.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 122227/122227 [04:08<00:00, 492.75it/s]\n",
      "  0%|                                                                             | 106/85730 [00:00<02:40, 532.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 85730/85730 [02:42<00:00, 529.13it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_lang_blocks_train = defaultdict()\n",
    "for language in lang_blocks_train:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for b in tqdm(range(len(lang_blocks_train[language]))):\n",
    "        #print(b)\n",
    "        block = lang_blocks_train[language][b]\n",
    "        encoded_list.append(parse_encode_text_block(block, encoded_trigrams))\n",
    "    encoded_lang_blocks_train[language] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsave(encoded_lang_blocks_train, 'encoded_lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-135,   89,  -39,   45,  -83,  -69, -125,   53,   11,  -63,   21,\n",
       "         -9,   59,   -9,  -15,  -29,   35,  -91, -119,    9,  101,   65,\n",
       "        -13,   -3,    7,  -99,   77,  -29,   75,   63,   27,  -11,   39,\n",
       "         49, -137,  -49,  -47,   29, -105,    9,   45,  -61,  163,   35,\n",
       "          1,   -9,  -17,    5,   39,  -41,    5,  -29,  -27,   51,  147,\n",
       "         -1,  -51,   77,  101,    9,   67,  103,  -85,   71,   67,  137,\n",
       "         33,   35,  -61,    7,    5,   31,   65,   -9,    7,  -57,  -29,\n",
       "        -15, -105,    1,  -41,  -53, -119,  -51,  -73,  -27,   -9,  -75,\n",
       "         47,  -75,  -15,  -85,  125,  -93,  -19,  -49,  -43,  -11,   -5,\n",
       "        -23,   33,   -1,  105,   21,   55,   -1,  107,  -81,  -15,   37,\n",
       "        -89,    1,    7,   47,   71,   41,   47,    1,  -75,  141,  -73,\n",
       "        -47,   55,  -59,  -91,  -59,   -5,  -31, -107, -139,  -55,  -93,\n",
       "         81,  -45,   73,   35,  117,    5,   55,  -45,   55,   11,   87,\n",
       "         29,   -9,  135,   -9,   43,   -5,  109,  -57,  109,   47,   27,\n",
       "        -95,  -45,    9,  -25,   21,   69,  -25,    7,  -69,  -67,   79,\n",
       "        -43,  -19,  -69,  -19,  -41,   25,   11, -141,  -83,   67,  -41,\n",
       "        -39,   61,   95,  -19,   41,  -25,   -3,   81,  -27,  -17,  -55,\n",
       "        -85,  -15,   -7,   29,    3,  -13,   -3,   83,  -23,  -55,  -17,\n",
       "         -9,  101,   75,   23,   -7,  -15,   25,    3,   37,   65,   77,\n",
       "         41,  -49,  -67,  -13,   81,   25,   63,   73,   69,    5,  -79,\n",
       "         61,  -85,  -63,   49,  -11,  -17,  -17,  -27,  -35,  -51,  -33,\n",
       "        -71,   97,   -7,   53,  -93,   37,  -43,  -47,   75,  105,   49,\n",
       "         37,   -5, -151,   39,   -7,   -7,   23,  -47,  -81,  -27,  -23,\n",
       "        -45,  -65,    7,  115,    5,    7,  -33, -101, -101,    5,   13,\n",
       "        -23,    5,  -25,   39,   15,  -51,   27,  -97,   25,  -59,   15,\n",
       "        -19,  -45,  -21, -121,  -27,  -63,  -55,  -11,  -25,  -25,   81,\n",
       "          5,  -79,  -49,   35,  -39,   37,   -7,  -17,   23,  -35,  -35,\n",
       "        -11,    1, -127,   29,  -17,   83,   65,  -45,   -1,  -99,  -41,\n",
       "          3,  -79,   95,   -7,   43,  -15,   75,  -51,  -57,   -7,  -11,\n",
       "       -101,   41,   99, -115,  -97,    7,    7,  -89,   37,  -87,   73,\n",
       "        -67,   15,   31,    3,    1,  -95,   -5,  135,   71,   83,   69,\n",
       "         41,  -39,   45,  -37,   51,   53,  -65,  -45,  -47,  -21,  -39,\n",
       "        147,  -43,   39,   77, -143,   63,   39,  -17,  -37,   69,   11,\n",
       "         41,   15,   57, -135,   17,  -25,    3,  145,  -35,   -1,  -85,\n",
       "        -31,  -25,   23,  -47,  -33,  105,  -99,    3,  -33,    3,   25,\n",
       "         45,   83,   17,   -1,  -13,  -43,   49,   37,  -37,   -9, -111,\n",
       "         65,   57,   49,   71,   -5,  -55,  -11,  -31,  -15,    5,   59,\n",
       "         -7,   -5,   15,   17, -135,   61,   37,   25,   87,  -59,   27,\n",
       "          3,   93,  117, -121,  -71,    1,  -13,  -13,   85,    5,    9,\n",
       "         45, -109,  -43,   25,  -43,  145,    9,    7,  -25,   27,  -31,\n",
       "         81,  -71,  -51,  -19, -117,  123,   29,  -75,   19,   -9,   91,\n",
       "        -75,   -1,    3,   43,   61,  -33,  -35,  115,  -19,    1,   25,\n",
       "        -67,  -15,  -57,  -59,   17,  -39,  105,   15,    9,    9,   17,\n",
       "       -109,   33,   81,  -41,  -57,    5,  -57,    9,   61,  -79,    3,\n",
       "        101,  -97,  -43,   35,  -21,  -27,  -11,   41,  161,   -1,   21,\n",
       "         69,  -77,  105,   -5,   31,   -3,   35,   23,  -41,  -97,  -37,\n",
       "         53,   -7,  -33,  -67,  -41,   83,  -71,   49,   35,   -1,  -59,\n",
       "       -141,   83,    9,  -19,  -59,  -25,  -11,   41,   11,   21, -115,\n",
       "          5,   29,  -15,    1,   -3,   77,   23,  139,   49,   11,   59,\n",
       "         53,   87,   17,  -45,   43,   25,  -41,   -9,   47,  -35,  -17,\n",
       "        -25,  -65,  159,   77,   27,  -55,   31,   63,   33,  -67,  -29,\n",
       "         61,   59,   31,   23,   73,   47,   23,   49,  -87,  -13,   91,\n",
       "          3,   -1,  123,  -89,   39,  -59,   41,   -3,   93,  -79,  -31,\n",
       "        -55,   53,  -33,    7,  -13,   53,   -3,   -3,  -33,   63,  -15,\n",
       "         19,   79,   25,  -93,   25,  -87,  -91,  -65,    5,   33,   33,\n",
       "        111,   77,  -11,  -33,  -17,  -83,   83,   13,   -1,  -11,  -53,\n",
       "         59,   33,    3,   13,   -9,  205,   19,   73,  -49,    5,   27,\n",
       "          9,   51,   83,   81,    1,   47,  -41,   79,  -29,   39,    9,\n",
       "         47,  -13,   35,  143,  -23,   75,   13,   71,   15,  -33,   35,\n",
       "          7,   29,  145,  -15,  -67,   37,  -31,  137,    3,   15,  -59,\n",
       "        167,   27,  -29,   63,   35,  -75,   27,   19,   17,  129,    3,\n",
       "        121,   51,   21,    1,  -37,   13,   -3,    7,  -13,  -47,  231,\n",
       "         57,   35,  -21,   59,  -27,   31,   49,   33,   41,   31,   79,\n",
       "         35,  -45,   83,  -55,    9,   89, -125,  -13,   39,   11,  -27,\n",
       "        -19,   59,    9,   27,    1,  -63,   45,   15,   25,  101, -103,\n",
       "          5,  -23,   23,  -17,   37,  -17,   53,    1,   -1,   91,   -3,\n",
       "         43,  -31, -133,   57,   19,  -19,  -35,   71,   15,  -57,   15,\n",
       "        -51,  -23,   33,  -37,   21,   -9,   -5,   -9,  -33,    3,  -17,\n",
       "        -51,   33,  -37,   23,  -33,  -35,   11,   73,    5,  107,   61,\n",
       "        125,  -29,  -39,  -15,  -69,  -85,   19,  -27,  -35,    9,  -43,\n",
       "         49,  -41,   21,   57,  -71,   31,  -75,   53,  -33,  -23,   79,\n",
       "          5,   51,  -29,   -9,  -57,  -59,  -45,  -77,    7,   21,   77,\n",
       "        -65, -185,  -15,  -13, -123,   39,  -79,   59,   49,    3,   -7,\n",
       "         35,  117,   15,  -37,  -23,   83,   15,   47,    1,   25,    9,\n",
       "         33,   25,   65,   65,   17,   79,   71,  -59,   77,  -21,   29,\n",
       "         27,   13,   97,   45,  105,  139,   -3,  -43,  -81,   49,   -3,\n",
       "         15,  -31,   37, -153,   27,   -9,   39,   53,   73,  -67,  -51,\n",
       "         81,  -71,   59,  -31,   33,    5,    1,  141,    7,   27,  -27,\n",
       "         27,  -89,  -57,  -25,   29,    3,  105,   81,   11,  -25,   87,\n",
       "         13,  -25,   75,   53,  -17,  -27,   31,  109,  127,    7,  -17,\n",
       "         89,  -49,  -47,   15,  -21,   -5,  -67,  -15,   -9,   51,   15,\n",
       "        -49,   -9,   -9,  -61,  -17,   23,  -79,  -55,    3,   15,   85,\n",
       "        -11,   49,   53,   49,   65,  -63,  -11,   19,   35,  -11,  -59,\n",
       "         43,   31,    1,  -47,   37,   21, -103, -153,   29,    3,   21,\n",
       "        -83,   49,  105,   73,   35,   63,   -7,   29,  -11,   75,  -13,\n",
       "        -75,  -85,  -63,   -9,  -85,   75,   21,   49,   73,   97,  -75,\n",
       "         -5,   45,   19,  -15,    9,   53,   29,  -53,   11,  -11,   13,\n",
       "        -73,  -35,   93,  -29,   43, -147,  -95,   77,  -25,   15,   49,\n",
       "         -3,   21,   11,  -79,  -89,  -63,  -65,  -39,  -53,  -19,  -11,\n",
       "          1,  -71,   75,   29,  -41,  -15,   69,  -11,   51,  -43,  -25,\n",
       "        -35,  -61,  -37,   15,  121,   83,   17,   33,  -17,  -55])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_lang_blocks_train['bul'][1052]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentenses_eurpar(path_to_file):\n",
    "    alph = alphabet()\n",
    "    \n",
    "    raw_lines = []\n",
    "    \n",
    "    with open(path_to_file, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            raw_lines.append(unidecode.unidecode(line[:-1].lower()))\n",
    "    \n",
    "    lines_final = []\n",
    "    for i in tqdm(range(len(raw_lines))):\n",
    "        line = raw_lines[i]\n",
    "        line = [x for x in ''.join(line).split(' ') if x]\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            word = [x for x in word if x in alph]\n",
    "            new_line.append(''.join(word).lower())\n",
    "        lines_final.append(new_line)\n",
    "    lines_final = [x for x in lines_final if x]\n",
    "    lines_final = [x for x in lines_final if x != ['']]\n",
    "    del raw_lines, line\n",
    "    return lines_final\n",
    "\n",
    "def create_blocks_eurpar(lines):    \n",
    "    blocks = []\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = ' '.join(lines[i])\n",
    "        blocks.append(line)\n",
    "    del line\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_root = os.path.join('data', 'test')\n",
    "data_lang_paths = os.listdir(test_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg-en',\n",
       " 'cs-en',\n",
       " 'da-en',\n",
       " 'de-en',\n",
       " 'el-en',\n",
       " 'en-en',\n",
       " 'es-en',\n",
       " 'et-en',\n",
       " 'fi-en',\n",
       " 'fr-en',\n",
       " 'hu-en',\n",
       " 'it-en',\n",
       " 'lt-en',\n",
       " 'lv-en',\n",
       " 'pl-en',\n",
       " 'ro-en']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths_test = defaultdict()\n",
    "for path in data_lang_paths:\n",
    "    lang_paths_test[path[:2]] = os.path.join(test_data_root, path, 'europarl-v7.'+path+'.'+path[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'bg': 'data\\\\test\\\\bg-en\\\\europarl-v7.bg-en.bg',\n",
       "             'cs': 'data\\\\test\\\\cs-en\\\\europarl-v7.cs-en.cs',\n",
       "             'da': 'data\\\\test\\\\da-en\\\\europarl-v7.da-en.da',\n",
       "             'de': 'data\\\\test\\\\de-en\\\\europarl-v7.de-en.de',\n",
       "             'el': 'data\\\\test\\\\el-en\\\\europarl-v7.el-en.el',\n",
       "             'en': 'data\\\\test\\\\en-en\\\\europarl-v7.en-en.en',\n",
       "             'es': 'data\\\\test\\\\es-en\\\\europarl-v7.es-en.es',\n",
       "             'et': 'data\\\\test\\\\et-en\\\\europarl-v7.et-en.et',\n",
       "             'fi': 'data\\\\test\\\\fi-en\\\\europarl-v7.fi-en.fi',\n",
       "             'fr': 'data\\\\test\\\\fr-en\\\\europarl-v7.fr-en.fr',\n",
       "             'hu': 'data\\\\test\\\\hu-en\\\\europarl-v7.hu-en.hu',\n",
       "             'it': 'data\\\\test\\\\it-en\\\\europarl-v7.it-en.it',\n",
       "             'lt': 'data\\\\test\\\\lt-en\\\\europarl-v7.lt-en.lt',\n",
       "             'lv': 'data\\\\test\\\\lv-en\\\\europarl-v7.lv-en.lv',\n",
       "             'pl': 'data\\\\test\\\\pl-en\\\\europarl-v7.pl-en.pl',\n",
       "             'ro': 'data\\\\test\\\\ro-en\\\\europarl-v7.ro-en.ro'})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_paths_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 406934/406934 [00:13<00:00, 29205.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 646605/646605 [00:19<00:00, 33966.53it/s]\n",
      " 82%|████████████████████████████████████████████████████████▉            | 1623299/1968800 [00:49<00:11, 31073.75it/s]"
     ]
    }
   ],
   "source": [
    "lines_raw_test = defaultdict()\n",
    "for lang in lang_paths_test:\n",
    "    lines_raw_test[lang] = process_sentenses_eurpar(lang_paths_test[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_blocks_test = defaultdict()\n",
    "for lang in lines_raw_test:\n",
    "    lang_blocks_test[lang] = create_blocks_eurpar(lines_raw_test[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lang_blocks_test = defaultdict()\n",
    "for language in lang_blocks_test:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for b in tqdm(range(len(lang_blocks_test[language]))):\n",
    "        #print(b)\n",
    "        block = lang_blocks_test[language][b]\n",
    "        encoded_list.append(parse_encode_text_block(block, encoded_trigrams))\n",
    "    encoded_lang_blocks_test[language] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "jobsave(encoded_lang_blocks_test, 'encoded_lang_blocks_test.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lang_blocks_train = jobload('encoded_lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
