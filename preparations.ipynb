{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import itertools\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import cmath\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_base(alphabet, n_order):\n",
    "    assert type(alphabet) == list, 'Alphabet is not list.'\n",
    "    assert len(set(alphabet)) == len(alphabet), 'Alphabet is not correct.'\n",
    "    alphabet.sort()\n",
    "    n_gram_base = set()\n",
    "    for combination in itertools.combinations_with_replacement(alphabet, n_order):\n",
    "        for permutation in itertools.permutations(combination, n_order):\n",
    "            n_gram = ''\n",
    "            for letter in permutation:\n",
    "                n_gram += letter\n",
    "            n_gram_base.add(n_gram)\n",
    "    assert len(n_gram_base) == len(alphabet)**n_order, 'Incorrect result'\n",
    "    print(\"Created\", str(n_order)+\"-gram base with\", len(n_gram_base), str(n_order)+\"-grams.\")\n",
    "    n_gram_base = list(n_gram_base)\n",
    "    n_gram_base.sort()\n",
    "    return n_gram_base\n",
    "\n",
    "def alphabet():\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    alphabet.append(' ')\n",
    "    return alphabet\n",
    "\n",
    "def process_sentenses(path_to_file):\n",
    "    alph = alphabet()\n",
    "    delimiters = \"\\t\", \"\\n\"\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    regexPattern\n",
    "\n",
    "    raw_lines = []\n",
    "    with open(path_to_file, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            raw_lines.append(line)\n",
    "        \n",
    "    lines_split = [re.split(regexPattern, line)[1].lower() for line in raw_lines]\n",
    "    \n",
    "    lines_final = []\n",
    "\n",
    "    for i in tqdm(range(len(lines_split))):\n",
    "        line = lines_split[i]\n",
    "        line_ = [unidecode.unidecode(x) for x in line if x.isalpha() or x.isspace()]\n",
    "        \n",
    "        line_ = [x for x in ''.join(line_).split(' ') if x]\n",
    "        new_line = []\n",
    "        for word in line_:\n",
    "            word = [x for x in word if x in alph]\n",
    "            new_line.append(''.join(word).lower())\n",
    "        lines_final.append(new_line)\n",
    "    del delimiters, regexPattern, raw_lines, lines_split, line, line_\n",
    "    return lines_final\n",
    "\n",
    "def create_blocks(lines):    \n",
    "    blocks = []\n",
    "    current_block = ''\n",
    "    \n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = ' '.join(lines[i])\n",
    "        if len(current_block) < 1000:\n",
    "            if len(current_block) > 0:\n",
    "                if current_block[-1] != ' ': current_block += ' '\n",
    "            current_block += line\n",
    "        elif len(current_block) >= 1000:\n",
    "            blocks.append(current_block[:1000])\n",
    "            current_block = line\n",
    "    del current_block, line\n",
    "    return blocks\n",
    "\n",
    "def base_26_latin_ngrams(n):\n",
    "    alb = alphabet()\n",
    "    base_ngrams = create_ngram_base(alb, n)\n",
    "    del alb\n",
    "    return base_ngrams\n",
    "\n",
    "def create_rand_atom_vect(order=1000):\n",
    "    atom = np.random.randint(-1, 1, order)\n",
    "    atom = np.where(atom==0, 1, atom)\n",
    "    atom.reshape(-1,1)\n",
    "    return atom\n",
    "\n",
    "def create_item_memory(order=1000, alphabet_size=27):\n",
    "    item_memory = []\n",
    "    for i in range(alphabet_size):\n",
    "        item_memory.append(create_rand_atom_vect(order))\n",
    "    item_memory = np.array(item_memory)\n",
    "    return item_memory    \n",
    "\n",
    "def create_fixed_permutations(order=1000, n=3):\n",
    "    index = [i for i in range(order)]\n",
    "    permutations = []\n",
    "    for i in range(n):\n",
    "        np.random.shuffle(index)\n",
    "        permutations.append(index.copy())\n",
    "    return permutations\n",
    "\n",
    "def bind(hdv1, hdv2):\n",
    "    binding = hdv1*hdv2\n",
    "    return binding\n",
    "    \n",
    "def permute(hdv, permutation, n_times_to_permute):\n",
    "    for i in range(n_times_to_permute):\n",
    "        hdv = hdv[permutation]\n",
    "    return hdv\n",
    "    \n",
    "def bundle(hdv1, hdv2):\n",
    "    bundling = hdv1 + hdv2\n",
    "    return bundling\n",
    "\n",
    "def measure_cosine_similarity(hdv1, hdv2):\n",
    "    return(cosine_similarity([hdv1], [hdv2]))\n",
    "\n",
    "def jobsave(object_to_save, filename):\n",
    "    joblib.dump(object_to_save, filename)\n",
    "\n",
    "def jobload(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root = os.path.join('data', 'train')\n",
    "data_lang_paths = os.listdir(train_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths = defaultdict()\n",
    "for path in data_lang_paths:\n",
    "    lang_paths[path[:3]] = os.path.join(train_data_root, path, path+'-sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_raw = defaultdict()\n",
    "for lang in lang_paths:\n",
    "    lines_raw[lang] = process_sentenses(lang_paths[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_blocks = defaultdict()\n",
    "for lang in lines_raw:\n",
    "    lang_blocks[lang] = create_blocks(lines_raw[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "fname = 'lang_blocks_train.jbl'\n",
    "jobsave(lang_blocks, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fname = 'lang_blocks_train.jbl'\n",
    "lang_blocks_train = jobload(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Encode train data using HD vectors, 3-grams and 21 languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 create latin alphabet and item memory using 27 atom HD binary (-1, 1) vectors with size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 26 letter + 1 space symbol alphabet:\n",
    "latin_alphabet = alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create item memory\n",
    "item_memory = create_item_memory(order=1000, alphabet_size=27)\n",
    "\n",
    "# check the shape; sum of item memory can show if some bias exist in random destribution\n",
    "item_memory.shape, np.sum(item_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 create fixed permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_permutations = create_fixed_permutations(order=1000, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 encode hd vector representations for all possible 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ngrams = base_26_latin_ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ngram(ngram):\n",
    "    vectors = []\n",
    "    for order in range(len(ngram)):\n",
    "        letter = ngram[order]\n",
    "        item_id = latin_alphabet.index(letter)\n",
    "        \n",
    "        # this is atom vector for that simbol\n",
    "        item_itself = item_memory[item_id]\n",
    "        permutation_n = fixed_permutations[order]\n",
    "        permuted_hdv = permute(item_itself, permutation_n, 1)\n",
    "        vectors.append(permuted_hdv)\n",
    "    output_vector = vectors[0]\n",
    "    for hdv in vectors[1:]:\n",
    "        output_vector = bind(output_vector, hdv)\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_trigrams = defaultdict()\n",
    "for trigram in base_ngrams:\n",
    "    encoded_trigrams[trigram] = encode_ngram(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "jobsave(item_memory, 'item_memory.jbl')\n",
    "jobsave(fixed_permutations, 'fixed_permutations.jbl')\n",
    "jobsave(base_ngrams, 'base_ngrams.jbl')\n",
    "jobsave(encoded_trigrams, 'encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "item_memory = jobload('item_memory.jbl')\n",
    "fixed_permutations = jobload('fixed_permutations.jbl')\n",
    "base_ngrams = jobload('base_ngrams.jbl')\n",
    "encoded_trigrams = jobload('encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Unsing encoded n-grams, encode the training data blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_encode_text_block(data_block, encoded_trigrams):\n",
    "\n",
    "    tri_grams = [data_block[i:i+3] for i in range(len(data_block)-2)]\n",
    "    encoded_block_trigrams = []\n",
    "    for tg in tri_grams:\n",
    "        encoded_block_trigrams.append(encoded_trigrams[tg.lower()])\n",
    "    \n",
    "    enc_tg_start = encoded_block_trigrams[0]\n",
    "    for enc_tg in encoded_block_trigrams:\n",
    "        enc_tg_start = bundle(enc_tg_start, enc_tg)\n",
    "    return enc_tg_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lang_blocks_train = defaultdict()\n",
    "for language in lang_blocks_train:\n",
    "    print(language)\n",
    "    encoded_list = parse_encode_text_block(lang_blocks_train[language][0], encoded_trigrams)\n",
    "    for b in tqdm(range(1, len(lang_blocks_train[language]))):\n",
    "        #print(b)\n",
    "        block = lang_blocks_train[language][b]\n",
    "        encoded_list = bundle(encoded_list , parse_encode_text_block(block, encoded_trigrams))\n",
    "    encoded_lang_blocks_train[language] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsave(encoded_lang_blocks_train, 'encoded_lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lang_blocks_train['bul'][1052]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentenses_eurpar(path_to_file):\n",
    "    alph = alphabet()\n",
    "    \n",
    "    _lines = []\n",
    "    \n",
    "    with open(path_to_file, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            new_line = ''.join([x for x in unidecode.unidecode(line[:-1].lower()) if x in latin_alphabet])\n",
    "            if len(new_line) > 0:\n",
    "                _lines.append(' '.join(new_line.split()))\n",
    "                \n",
    "    return _lines\n",
    "\n",
    "def process_sentenses_nn(path_to_file, nn):\n",
    "    alph = alphabet()\n",
    "    \n",
    "    _lines = []\n",
    "    \n",
    "    with open(path_to_file, 'r', encoding=\"utf-8\") as file:\n",
    "        counter = 0\n",
    "        for line in file:\n",
    "            if counter <nn+1:\n",
    "                new_line = ''.join([x for x in unidecode.unidecode(line[:-1].lower()) if x in latin_alphabet])\n",
    "                if len(new_line) > 0:\n",
    "                    _lines.append(' '.join(new_line.split()))\n",
    "                    counter+=1\n",
    "            else: \n",
    "                break\n",
    "\n",
    "    return _lines\n",
    "\n",
    "def create_blocks_eurpar(lines):    \n",
    "    blocks = []\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = ' '.join(lines[i])\n",
    "        blocks.append(line)\n",
    "    del line\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_root = os.path.join('data', 'test')\n",
    "data_lang_paths = os.listdir(test_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lang_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths_test = defaultdict()\n",
    "for path in data_lang_paths:\n",
    "    lang_paths_test[path[:2]] = os.path.join(test_data_root, path, 'europarl-v7.'+path+'.'+path[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_paths_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = defaultdict()\n",
    "for lang in lang_paths_test:\n",
    "    print(lang)\n",
    "    lines_test[lang] = process_sentenses_nn(lang_paths_test[lang], 3000)\n",
    "    \n",
    "jobsave(lines_test, 'lines_test_nn.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = defaultdict()\n",
    "for lang in lang_paths_test:\n",
    "    print(lang)\n",
    "    lines_test[lang] = process_sentenses_eurpar(lang_paths_test[lang])\n",
    "    \n",
    "jobsave(lines_test, 'lines_test_1000.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = jobload('lines_test.jbl')\n",
    "lines_test_2 = jobload('lines_test_2.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_encode_text_block(data_block, encoded_trigrams):\n",
    "\n",
    "    tri_grams = [data_block[i:i+3] for i in range(len(data_block)-2)]\n",
    "    encoded_block_trigrams = []\n",
    "    for tg in tri_grams:\n",
    "        encoded_block_trigrams.append(encoded_trigrams[tg])\n",
    "    \n",
    "    enc_tg_start = encoded_block_trigrams[0]\n",
    "    for enc_tg in encoded_block_trigrams:\n",
    "        enc_tg_start = bundle(enc_tg_start, enc_tg)\n",
    "    return enc_tg_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize([aaa], norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test_nn = jobload('lines_test_nn.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_trigrams['aaa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_blocks_test = defaultdict()\n",
    "for language in lines_test_nn:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for bl in tqdm(range(len(lines_test_nn[language]))):\n",
    "        block = lines_test[language][bl]\n",
    "        if len(block) >= 3:\n",
    "            encoded_list.append(normalize([parse_encode_text_block(block, encoded_trigrams)], norm='l2'))\n",
    "    root = 'encoded_langs_test_2'\n",
    "    filename = language +'_encoded_test.jbl'\n",
    "    path = os.path.join(root, filename)\n",
    "    jobsave(encoded_list, path)\n",
    "    del encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_blocks_test = defaultdict()\n",
    "for language in lines_test_2:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for bl in tqdm(range(len(lines_test_2[language]))):\n",
    "        #if bl%10000 == 0:\n",
    "        #    print(language, ':', bl)\n",
    "        block = lines_test_2[language][bl]\n",
    "        if len(block) >= 3:\n",
    "            encoded_list.append(normalize([parse_encode_text_block(block, encoded_trigrams)], norm='l2'))\n",
    "    filename = language +'_encoded_test.jbl'\n",
    "    jobsave(encoded_list, filename)\n",
    "    del encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "jobsave(encoded_blocks_test, 'encoded_blocks_test.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lang_blocks_train = jobload('encoded_lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = {\n",
    "    'bul' : 'Bulgarian',\n",
    "    'ces' : 'Czech',\n",
    "    'dan' : 'Danish',\n",
    "    'deu' : 'German',\n",
    "    'ell' : 'Greek',\n",
    "    'eng' : 'English',\n",
    "    'est' : 'Estonian',\n",
    "    'fin' : 'Finnish',\n",
    "    'fra' : 'French',\n",
    "    'hun' : 'Hungarian',\n",
    "    'ita' : 'Italian',\n",
    "    'lav' : 'Latvian',\n",
    "    'lit' : 'Lithuanian',\n",
    "    'nld' : 'Dutch',\n",
    "    'pol' : 'Polish',\n",
    "    'por' : 'Portuguese',\n",
    "    'ron' : 'Romanian',\n",
    "    'slk' : 'Slovak',\n",
    "    'slv' : 'Slovene',\n",
    "    'spa' : 'Spanish',\n",
    "    'swe' : 'Swedish'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_lang_train = defaultdict()\n",
    "for key in encoded_lang_blocks_train:\n",
    "    new_key = train_keys[key]\n",
    "    norm_vect = normalize([encoded_lang_blocks_train[key]], norm='l2')\n",
    "    nor_lang_train[new_key] = norm_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsave(nor_lang_train, 'nor_lang_train.jbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complex vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = jobload('lines_test.jbl')\n",
    "lines_test_2 = jobload('lines_test_2.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_counterpart(real):\n",
    "    img_pos = (1- real**2)**0.5\n",
    "    \n",
    "    return img_pos\n",
    "\n",
    "def create_complex_range_u1(decimals):\n",
    "    real_range = np.linspace(-1, 1, (10**decimals)*2+1)\n",
    "    complex_range = []\n",
    "    for x in real_range:\n",
    "        y = img_counterpart(x)\n",
    "        if y != 0:\n",
    "            complex_range.append(complex(x, y))\n",
    "            complex_range.append(complex(x, -y))\n",
    "        elif y ==0:\n",
    "            complex_range.append(complex(x, y))\n",
    "    complex_range = np.array(complex_range)\n",
    "    complex_range.sort()\n",
    "    return complex_range\n",
    "\n",
    "def create_random_complex_vector(source, order = 1000):\n",
    "    atom_vector = np.random.choice(source, order)\n",
    "    return atom_vector\n",
    "\n",
    "def bind_comp(hdv1, hdv2):\n",
    "    binding = hdv1*hdv2\n",
    "    return binding\n",
    "    \n",
    "def permute_comp(hdv, place):\n",
    "    np.roll(hdv, place)\n",
    "    return hdv\n",
    "    \n",
    "def bundle_comp(hdv1, hdv2):\n",
    "    bundling = hdv1 + hdv2\n",
    "    return bundling\n",
    "\n",
    "def comp_cos_sim(hdv1, hdv2):\n",
    "    return(cosine_similarity([hdv1.real], [hdv2.real]))\n",
    "\n",
    "def create_comp_item_memory(order=1000, alphabet_size=27):\n",
    "    item_memory = []\n",
    "    for i in range(alphabet_size):\n",
    "        item_memory.append(create_random_complex_vector(complex_source, order))\n",
    "    item_memory = np.array(item_memory)\n",
    "    return item_memory    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_source = create_complex_range_u1(6)\n",
    "len(complex_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_alphabet = alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3-gram base with 19683 3-grams.\n"
     ]
    }
   ],
   "source": [
    "base_ngrams = base_26_latin_ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_source = create_complex_range_u1(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_item_memory = create_comp_item_memory(order=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_comp_ngram(ngram):\n",
    "    vectors = []\n",
    "    for order in range(len(ngram)):\n",
    "        letter = ngram[order]\n",
    "        item_id = latin_alphabet.index(letter)\n",
    "\n",
    "        item_itself = comp_item_memory[item_id]\n",
    "\n",
    "        permuted_hdv = permute_comp(item_itself, order)\n",
    "        vectors.append(permuted_hdv)\n",
    "    output_vector = vectors[0]\n",
    "    for hdv in vectors[1:]:\n",
    "        output_vector = bind(output_vector, hdv)\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_encoded_trigrams = defaultdict()\n",
    "for trigram in base_ngrams:\n",
    "    comp_encoded_trigrams[trigram] = encode_comp_ngram(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsave(comp_encoded_trigrams, 'comp_encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "jobsave(comp_item_memory, 'comp_item_memory.jbl')\n",
    "jobsave(comp_encoded_trigrams, 'comp_encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "comp_item_memory = jobload('comp_item_memory.jbl')\n",
    "base_ngrams = jobload('base_ngrams.jbl')\n",
    "comp_encoded_trigrams = jobload('encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_blocks_train = jobload('lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode text blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_encode_text_block(data_block, encoded_trigrams):\n",
    "\n",
    "    tri_grams = [data_block[i:i+3] for i in range(len(data_block)-2)]\n",
    "    encoded_block_trigrams = []\n",
    "    for tg in tri_grams:\n",
    "        encoded_block_trigrams.append(encoded_trigrams[tg.lower()])\n",
    "    \n",
    "    enc_tg_start = encoded_block_trigrams[0]\n",
    "    for enc_tg in encoded_block_trigrams:\n",
    "        enc_tg_start = bundle(enc_tg_start, enc_tg)\n",
    "    return enc_tg_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_encoded_lang_blocks_train = defaultdict()\n",
    "for language in lang_blocks_train:\n",
    "    new_language = train_keys[language]\n",
    "    print(language)\n",
    "    print(new_language)\n",
    "    encoded_list = parse_encode_text_block(lang_blocks_train[language][0], comp_encoded_trigrams)\n",
    "    for b in tqdm(range(1, len(lang_blocks_train[language]))):\n",
    "        #print(b)\n",
    "        block = lang_blocks_train[language][b]\n",
    "        encoded_list = bundle(encoded_list , parse_encode_text_block(block, comp_encoded_trigrams))\n",
    "    comp_encoded_lang_blocks_train[new_language] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsave(comp_encoded_lang_blocks_train, 'comp_encoded_lang_blocks_train.jbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create complex test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = jobload('lines_test.jbl')\n",
    "lines_test_2 = jobload('lines_test_2.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_encoded_trigrams = jobload('comp_encoded_trigrams.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 403/1990304 [00:00<16:47, 1975.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1990304/1990304 [16:05<00:00, 2062.09it/s]\n",
      "  0%|          | 16/637649 [00:00<1:08:49, 154.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637649/637649 [03:57<00:00, 2684.34it/s]\n",
      "  0%|          | 1389/620457 [00:00<01:27, 7109.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620457/620457 [03:41<00:00, 2803.81it/s]\n",
      "  0%|          | 385/1852652 [00:00<18:47, 1643.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 113500/1852652 [00:41<11:03, 2621.71it/s]"
     ]
    }
   ],
   "source": [
    "#encoded_blocks_test = defaultdict()\n",
    "for language in lines_test_2:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for bl in tqdm(range(len(lines_test_2[language]))):\n",
    "\n",
    "        block = lines_test_2[language][bl]\n",
    "        if len(block) >= 3:\n",
    "            encoded_list.append(parse_encode_text_block(block, comp_encoded_trigrams))\n",
    "    filename = language +'comp_encoded_test.jbl'\n",
    "    root = 'comp_encoded_langs_test'\n",
    "    jobsave(encoded_list, os.path.join(root, filename))\n",
    "    del encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 121/404381 [00:00<05:35, 1206.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404381/404381 [02:40<00:00, 2517.48it/s]\n",
      "  0%|          | 831/643491 [00:00<01:17, 8294.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643491/643491 [03:34<00:00, 2997.68it/s]\n",
      "  0%|          | 251/1961219 [00:00<13:03, 2501.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1961219/1961219 [12:37<00:00, 2589.63it/s]\n",
      "  0%|          | 3/1915852 [00:00<17:46:24, 29.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1915852/1915852 [14:17<00:00, 2234.71it/s]\n",
      "  0%|          | 3/1226281 [00:00<11:56:07, 28.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1226281/1226281 [08:54<00:00, 2294.77it/s]\n",
      "  0%|          | 772/396827 [00:00<00:51, 7697.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396827/396827 [02:23<00:00, 2770.86it/s]\n",
      "  0%|          | 260/1957136 [00:00<12:34, 2592.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1957136/1957136 [13:39<00:00, 2387.31it/s]\n",
      "  0%|          | 16/648655 [00:00<1:10:29, 153.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648655/648655 [04:00<00:00, 2698.12it/s]\n",
      "  0%|          | 442/1917127 [00:00<14:30, 2202.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1917127/1917127 [14:21<00:00, 2225.82it/s]\n",
      "  0%|          | 4/2000544 [00:00<14:45:04, 37.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000544/2000544 [14:40<00:00, 2270.99it/s]\n",
      "  0%|          | 7/621856 [00:00<2:31:02, 68.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 621856/621856 [04:13<00:00, 2448.60it/s]\n",
      "  0%|          | 441/1899872 [00:00<14:11, 2231.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899872/1899872 [15:58<00:00, 1982.77it/s]\n",
      "  0%|          | 16/631187 [00:00<1:06:32, 158.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 631187/631187 [03:42<00:00, 2841.86it/s]\n",
      "  0%|          | 1420/635078 [00:00<01:26, 7341.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 635078/635078 [04:05<00:00, 2589.36it/s]\n",
      "  0%|          | 436/629205 [00:00<02:24, 4358.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629205/629205 [04:15<00:00, 2460.63it/s]\n",
      "  0%|          | 245/1952785 [00:00<13:19, 2440.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952785/1952785 [15:20<00:00, 2122.55it/s]\n",
      "  0%|          | 19/396797 [00:00<35:02, 188.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396797/396797 [02:51<00:00, 2315.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#encoded_blocks_test = defaultdict()\n",
    "for language in lines_test:\n",
    "    print(language)\n",
    "    encoded_list = []\n",
    "    for bl in tqdm(range(len(lines_test[language]))):\n",
    "\n",
    "        block = lines_test[language][bl]\n",
    "        if len(block) >= 3:\n",
    "            encoded_list.append(parse_encode_text_block(block, comp_encoded_trigrams))\n",
    "    filename = language +'comp_encoded_test.jbl'\n",
    "    root = 'comp_encoded_langs_test'\n",
    "    jobsave(encoded_list, os.path.join(root, filename))\n",
    "    del encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
